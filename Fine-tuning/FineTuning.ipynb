{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "1 line per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent\n",
    "file = str(DirPpath) + \"\\DataCollection\\corpus.csv\"\n",
    "\n",
    "CorpusDF = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusDF.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusDF = CorpusDF.dropna()\n",
    "\n",
    "CorpusDF['0'] = CorpusDF['0'].apply(lambda x: x.replace('skip to main content this site uses cookies to offer you a better browsing experience. find out more on how we use cookies. accept all cookies accept only essential cookies an official website of the european union', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for ID, content in zip(CorpusDF['Unnamed: 0'].values, CorpusDF['0'].values):\n",
    "        f.write('\\n'.join([str(ID), content]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ecaudron001\\Documents\\GitHub\\LLM-for-Tourism\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import transformers\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "# LOCAL_INPUT_PATH is mapped to S3 input location for covid news articles \n",
    "LOCAL_INPUT_PATH = 'corpus.txt' \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the custom vocabulary after training the tokenizer\n",
    "LOCAL_OUTPUT_PATH = 'vocab'\n",
    "VOCAB_SIZE = 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = 'corpus.txt'\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(files=paths, vocab_size=VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning\\\\vocab.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent\n",
    "OutputPath = str(DirPpath) + \"\\Fine-tuning\"\n",
    "\n",
    "tokenizer.save_model(OutputPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating custom tokenizer\n",
      "Test sentence: covid is a virus\n",
      "Encoded sentence: ['[CLS]', 'covid', 'is', 'a', 'virus', '[SEP]']\n",
      "Token ID for token (covid) = 1240\n",
      "Vocabulary size = 30522\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer('vocab.txt')\n",
    "\n",
    "# Evaluate custom tokenizer \n",
    "print('Evaluating custom tokenizer')\n",
    "test_sentence = 'covid is a virus'\n",
    "print(f'Test sentence: {test_sentence}')\n",
    "tokens = tokenizer.encode(test_sentence).tokens\n",
    "print(f'Encoded sentence: {tokens}')\n",
    "token_id = tokenizer.token_to_id('covid')\n",
    "print(f'Token ID for token (covid) = {token_id}')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f'Vocabulary size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess MLM custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertConfig\n",
    "from pathlib import Path\n",
    "import transformers \n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "# LOCAL_INPUT_PATH is mapped to S3 input location for covid news articles \n",
    "LOCAL_INPUT_PATH = '/opt/ml/processing/input' \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the processed input data (COVID articles)\n",
    "LOCAL_OUTPUT_PATH = 'c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning'\n",
    "MAX_LENGTH = 512\n",
    "CHUNK_SIZE = 128\n",
    "N_GPUS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating BERT tokenizer using custom vocabulary from [vocab.txt]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bos_token, but it is not set yet.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: BertTokenizerFast(name_or_path='c:\\Users\\ecaudron001\\Documents\\GitHub\\LLM-for-Tourism\\Fine-tuning', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Re-create BERT WordPiece tokenizer using the saved custom vocabulary from the previous job\n",
    "config = BertConfig()\n",
    "print(f'Re-creating BERT tokenizer using custom vocabulary from [vocab.txt]')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning', config=config)\n",
    "tokenizer.model_max_length = MAX_LENGTH\n",
    "tokenizer.init_kwargs['model_max_length'] = MAX_LENGTH\n",
    "print(f'Tokenizer: {tokenizer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and collating input data to create mini batches for Masked Language Model (MLM) training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 158.55it/s]\n",
      "Generating train split: 178246 examples [00:00, 378683.94 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 178246\n",
      "})\n",
      "Splitting dataset into train and validation splits\n",
      "Data splits: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 160421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 17825\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Read dataset and collate to create mini batches for Masked Language Model (MLM) training\n",
    "print('Reading and collating input data to create mini batches for Masked Language Model (MLM) training')\n",
    "dataset = load_dataset('text', data_files='corpus.txt', split='train', cache_dir='/tmp/cache')\n",
    "print(f'Dataset: {dataset}')\n",
    "\n",
    "# Split dataset into train and validation splits \n",
    "print('Splitting dataset into train and validation splits')\n",
    "train_test_splits = dataset.train_test_split(shuffle=True, seed=123, test_size=0.1)\n",
    "data_splits = DatasetDict({'train': train_test_splits['train'], \n",
    "                           'validation': train_test_splits['test']})\n",
    "print(f'Data splits: {data_splits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset splits\n",
      "Total number of processes = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 160421/160421 [00:29<00:00, 5454.03 examples/s] \n",
      "Map (num_proc=8): 100%|██████████| 17825/17825 [00:18<00:00, 968.76 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 160421\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids'],\n",
      "        num_rows: 17825\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "def tokenize(article, tokenizer = tokenizer):\n",
    "    tokenized_article = tokenizer(article['text'])\n",
    "    if tokenizer.is_fast:\n",
    "        tokenized_article['word_ids'] = [tokenized_article.word_ids(i) for i in range(len(tokenized_article['input_ids']))]\n",
    "    return tokenized_article\n",
    "\n",
    "\n",
    "print('Tokenizing dataset splits')\n",
    "num_proc = int(os.cpu_count()/N_GPUS)\n",
    "print(f'Total number of processes = {num_proc}')\n",
    "tokenized_datasets = data_splits.map(tokenize, batched=True, num_proc=num_proc, remove_columns=['text'])\n",
    "print(f'Tokenized datasets: {tokenized_datasets}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating and chunking the datasets to a fixed length\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 160421/160421 [00:30<00:00, 5341.79 examples/s] \n",
      "Map (num_proc=8): 100%|██████████| 17825/17825 [00:16<00:00, 1059.03 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 20064\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 2204\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Concat and chunk dataset \n",
    "def concat_and_chunk(articles, CHUNK_SIZE = CHUNK_SIZE):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {key: sum(articles[key], []) for key in articles.keys()}\n",
    "    # Compute length of concatenated texts\n",
    "    total_length = len(concatenated_examples[list(articles.keys())[0]])\n",
    "    # We drop the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length//CHUNK_SIZE) * CHUNK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    chunked_articles = {key: [text[i : i+CHUNK_SIZE] for i in range(0, total_length, CHUNK_SIZE)] for key, text in concatenated_examples.items()}\n",
    "    # Create a new labels column\n",
    "    chunked_articles['labels'] = chunked_articles['input_ids'].copy()\n",
    "    return chunked_articles\n",
    "    \n",
    "print('Concatenating and chunking the datasets to a fixed length')\n",
    "chunked_datasets = tokenized_datasets.map(concat_and_chunk, batched=True, num_proc=num_proc)\n",
    "print(f'Chunked datasets: {chunked_datasets}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving chunked datasets to local disk c:\\Users\\ecaudron001\\Documents\\GitHub\\LLM-for-Tourism\\Fine-tuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/20064 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20064/20064 [00:00<00:00, 126617.64 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2204/2204 [00:00<00:00, 60287.51 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating if datasets were saved correctly\n",
      "Reloaded dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 20064\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'word_ids', 'labels'],\n",
      "        num_rows: 2204\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Save chunked datasets to local disk (EBS volume)\n",
    "print(f'Saving chunked datasets to local disk {LOCAL_OUTPUT_PATH}')\n",
    "chunked_datasets.save_to_disk(f'{LOCAL_OUTPUT_PATH}')\n",
    "\n",
    "# Validate if datasets were saved correctly\n",
    "print('Validating if datasets were saved correctly')\n",
    "reloaded_dataset = datasets.load_from_disk(f'{LOCAL_OUTPUT_PATH}')\n",
    "print(f'Reloaded dataset: {reloaded_dataset}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reloaded_dataset['train'][10][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
