{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset\n",
    "1 line per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent\n",
    "file = str(DirPpath) + \"\\DataCollection\\Files\\corpus.csv\"\n",
    "\n",
    "CorpusDF = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CorpusDF.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorpusDF = CorpusDF.dropna()\n",
    "\n",
    "CorpusDF['0'] = CorpusDF['0'].apply(lambda x: x.replace('skip to main content this site uses cookies to offer you a better browsing experience. find out more on how we use cookies. accept all cookies accept only essential cookies an official website of the european union', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = str(DirPpath) + \"\\Fine-tuning\\Files\\corpus.csv\"\n",
    "CorpusDF.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = str(DirPpath) + \"\\Fine-tuning\\Files\\corpus.txt\"\n",
    "with open(file, 'w', encoding='utf-8') as f:\n",
    "    for ID, content in zip(CorpusDF['Unnamed: 0'].values, CorpusDF['0'].values):\n",
    "        f.write('\\n'.join([str(ID), content]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract vocabulary: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import RobertaTokenizer\n",
    "import transformers\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "# LOCAL_INPUT_PATH is mapped to S3 input location for covid news articles \n",
    "LOCAL_INPUT_PATH = file \n",
    "# LOCAL_OUTPUT_PATH is mapped to S3 output location where we want to save the custom vocabulary after training the tokenizer\n",
    "LOCAL_OUTPUT_PATH =  str(DirPpath) + \"vocab\"\n",
    "VOCAB_SIZE = 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = file\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "tokenizer.train(files=paths, vocab_size=VOCAB_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning\\\\Files\\\\vocab.txt']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DirPpath = Path(os.path.abspath('')).parent\n",
    "OutputPath = str(DirPpath) + \"\\Fine-tuning\\Files\"\n",
    "\n",
    "tokenizer.save_model(OutputPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating custom tokenizer\n",
      "Test sentence: covid is a virus\n",
      "Encoded sentence: ['[CLS]', 'covid', 'is', 'a', 'virus', '[SEP]']\n",
      "Token ID for token (covid) = 976\n",
      "Vocabulary size = 27799\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertWordPieceTokenizer('vocab.txt')\n",
    "\n",
    "# Evaluate custom tokenizer \n",
    "print('Evaluating custom tokenizer')\n",
    "test_sentence = 'covid is a virus'\n",
    "print(f'Test sentence: {test_sentence}')\n",
    "tokens = tokenizer.encode(test_sentence).tokens\n",
    "print(f'Encoded sentence: {tokens}')\n",
    "token_id = tokenizer.token_to_id('covid')\n",
    "print(f'Token ID for token (covid) = {token_id}')\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "print(f'Vocabulary size = {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vocab.txt', delimiter=\"/t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"vocab.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning\\\\vocab.json',\n",
       " 'c:\\\\Users\\\\ecaudron001\\\\Documents\\\\GitHub\\\\LLM-for-Tourism\\\\Fine-tuning\\\\merges.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "\n",
    "paths = str(DirPpath) + \"\\Fine-tuning\\Files\\corpus.txt\"\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=8192, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"</s>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])\n",
    "#Save the Tokenizer to disk\n",
    "output = str(DirPpath) + \"\\Fine-tuning\"\n",
    "tokenizer.save_model(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ecaudron001\\AppData\\Local\\Temp\\ipykernel_16236\\3076094962.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('merges.txt', delimiter=\"/t\", header=None)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('merges.txt', delimiter=\"/t\", header=None)\n",
    "df.to_csv(\"merges.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
